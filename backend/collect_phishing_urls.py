#!/usr/bin/env python3
"""
ä»åˆæ³•çš„å®‰å…¨æ•°æ®åº“æ”¶é›†çœŸå®é’“é±¼ç½‘ç«™URLç”¨äºæµ‹è¯•

æ•°æ®æ¥æºï¼š
- OpenPhish: https://openphish.com/
- PhishTank: https://phishtank.org/
- URLhaus: https://urlhaus.abuse.ch/
"""

import requests
import json
import time
import re
from urllib.parse import urlparse
from typing import List, Dict
import os
import sys

class PhishingURLCollector:
    """é’“é±¼URLæ”¶é›†å™¨"""

    def __init__(self):
        self.sources = {
            'openphish': {
                'url': 'https://openphish.com/feed.txt',
                'name': 'OpenPhish'
            },
            'phishing_database': {
                'url': 'https://raw.githubusercontent.com/Phishing-Database/Phishing.Database/master/phishing-links-ACTIVE-NOW.txt',
                'name': 'Phishing.Database'
            },
            'urlhaus': {
                'url': 'https://urlhaus.abuse.ch/downloads/text_online/',
                'name': 'URLhaus'
            }
        }

    def collect_from_openphish(self) -> List[str]:
        """ä»OpenPhishæ”¶é›†URL"""
        try:
            print("ğŸ”„ æ­£åœ¨ä» OpenPhish æ”¶é›†æ•°æ®...")
            response = requests.get('https://openphish.com/feed.txt', timeout=30)
            response.raise_for_status()

            urls = []
            for line in response.text.strip().split('\n'):
                if line.strip() and self.is_valid_url(line.strip()):
                    urls.append(line.strip())

            print(f"âœ… OpenPhish æ”¶é›†åˆ° {len(urls)} ä¸ªURL")
            return urls[:100]  # é™åˆ¶æ•°é‡

        except Exception as e:
            print(f"âŒ OpenPhish æ”¶é›†å¤±è´¥: {e}")
            return []

    def collect_from_phishing_database(self) -> List[str]:
        """ä»Phishing.Databaseæ”¶é›†URL"""
        try:
            print("ğŸ”„ æ­£åœ¨ä» Phishing.Database æ”¶é›†æ•°æ®...")
            response = requests.get(
                'https://raw.githubusercontent.com/Phishing-Database/Phishing.Database/master/phishing-links-ACTIVE-NOW.txt',
                timeout=30
            )
            response.raise_for_status()

            urls = []
            for line in response.text.strip().split('\n'):
                if line.strip() and self.is_valid_url(line.strip()):
                    urls.append(line.strip())

            print(f"âœ… Phishing.Database æ”¶é›†åˆ° {len(urls)} ä¸ªURL")
            return urls[:100]

        except Exception as e:
            print(f"âŒ Phishing.Database æ”¶é›†å¤±è´¥: {e}")
            return []

    def collect_from_urlhaus(self) -> List[str]:
        """ä»URLhausæ”¶é›†URL"""
        try:
            print("ğŸ”„ æ­£åœ¨ä» URLhaus æ”¶é›†æ•°æ®...")
            response = requests.get('https://urlhaus.abuse.ch/downloads/text_online/', timeout=30)
            response.raise_for_status()

            urls = []
            lines = response.text.strip().split('\n')
            for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜è¡Œ
                if line.strip():
                    parts = line.split(',')
                    if len(parts) >= 1:
                        url = parts[0].strip()
                        if url and self.is_valid_url(url):
                            urls.append(url)

            print(f"âœ… URLhaus æ”¶é›†åˆ° {len(urls)} ä¸ªURL")
            return urls[:100]

        except Exception as e:
            print(f"âŒ URLhaus æ”¶é›†å¤±è´¥: {e}")
            return []

    def is_valid_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼"""
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False

    def categorize_urls(self, urls: List[str]) -> Dict[str, List[str]]:
        """æŒ‰ç±»å‹åˆ†ç±»URL"""
        categories = {
            'financial': [],  # é‡‘èç±»
            'social_media': [],  # ç¤¾äº¤åª’ä½“ç±»
            'shopping': [],  # è´­ç‰©ç±»
            'government': [],  # æ”¿åºœç±»
            'tech_company': [],  # ç§‘æŠ€å…¬å¸ç±»
            'other': []  # å…¶ä»–
        }

        # å…³é”®è¯åˆ†ç±»
        financial_keywords = ['bank', 'paypal', 'payment', 'card', 'credit', 'money', 'financial', 'secure']
        social_keywords = ['facebook', 'instagram', 'twitter', 'linkedin', 'social', 'profile']
        shopping_keywords = ['amazon', 'shop', 'store', 'buy', 'order', 'shipping']
        government_keywords = ['irs', 'government', 'tax', 'official', 'gov']
        tech_keywords = ['microsoft', 'apple', 'google', 'support', 'help', 'account']

        for url in urls:
            url_lower = url.lower()
            categorized = False

            if any(keyword in url_lower for keyword in financial_keywords):
                categories['financial'].append(url)
                categorized = True
            elif any(keyword in url_lower for keyword in social_keywords):
                categories['social_media'].append(url)
                categorized = True
            elif any(keyword in url_lower for keyword in shopping_keywords):
                categories['shopping'].append(url)
                categorized = True
            elif any(keyword in url_lower for keyword in government_keywords):
                categories['government'].append(url)
                categorized = True
            elif any(keyword in url_lower for keyword in tech_keywords):
                categories['tech_company'].append(url)
                categorized = True

            if not categorized:
                categories['other'].append(url)

        return categories

    def collect_all(self) -> Dict[str, any]:
        """ä»æ‰€æœ‰æ¥æºæ”¶é›†æ•°æ®"""
        print("ğŸš€ å¼€å§‹æ”¶é›†é’“é±¼ç½‘ç«™æ•°æ®...")

        all_urls = []
        sources_data = {}

        # ä»å„ä¸ªæ¥æºæ”¶é›†
        openphish_urls = self.collect_from_openphish()
        if openphish_urls:
            all_urls.extend(openphish_urls)
            sources_data['openphish'] = {
                'count': len(openphish_urls),
                'urls': openphish_urls[:20]  # åªä¿å­˜å‰20ä¸ªä½œä¸ºç¤ºä¾‹
            }

        phishing_db_urls = self.collect_from_phishing_database()
        if phishing_db_urls:
            all_urls.extend(phishing_db_urls)
            sources_data['phishing_database'] = {
                'count': len(phishing_db_urls),
                'urls': phishing_db_urls[:20]
            }

        urlhaus_urls = self.collect_from_urlhaus()
        if urlhaus_urls:
            all_urls.extend(urlhaus_urls)
            sources_data['urlhaus'] = {
                'count': len(urlhaus_urls),
                'urls': urlhaus_urls[:20]
            }

        # å»é‡
        unique_urls = list(set(all_urls))
        print(f"ğŸ“Š æ€»è®¡æ”¶é›†åˆ° {len(unique_urls)} ä¸ªå”¯ä¸€URL")

        # åˆ†ç±»
        categories = self.categorize_urls(unique_urls[:200])  # åˆ†ç±»å‰200ä¸ª

        result = {
            'total_count': len(unique_urls),
            'sources': sources_data,
            'categories': categories,
            'sample_urls': unique_urls[:50],  # å‰50ä¸ªä½œä¸ºç¤ºä¾‹
            'collection_time': time.strftime('%Y-%m-%d %H:%M:%S')
        }

        return result

    def save_to_file(self, data: Dict, filename: str):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ° {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")

    def generate_test_urls(self, data: Dict) -> List[str]:
        """ç”Ÿæˆç”¨äºæµ‹è¯•çš„URLåˆ—è¡¨"""
        test_urls = []

        # ä»å„ä¸ªç±»åˆ«é€‰æ‹©URL
        for category, urls in data['categories'].items():
            if urls:
                test_urls.extend(urls[:10])  # æ¯ä¸ªç±»åˆ«å–10ä¸ª

        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°é‡
        while len(test_urls) < 100 and len(data['sample_urls']) > 0:
            remaining = 100 - len(test_urls)
            test_urls.extend(data['sample_urls'][:remaining])

        return test_urls[:100]

def main():
    """ä¸»å‡½æ•°"""
    collector = PhishingURLCollector()

    try:
        # æ”¶é›†æ•°æ®
        data = collector.collect_all()

        # ä¿å­˜å®Œæ•´æ•°æ®
        collector.save_to_file(data, 'phishing_urls_data.json')

        # ç”Ÿæˆæµ‹è¯•URLåˆ—è¡¨
        test_urls = collector.generate_test_urls(data)

        # ä¿å­˜æµ‹è¯•URL
        with open('phishing_test_urls.txt', 'w', encoding='utf-8') as f:
            for url in test_urls:
                f.write(url + '\n')

        print(f"âœ… ç”Ÿæˆäº† {len(test_urls)} ä¸ªæµ‹è¯•URL")
        print("ğŸ“ æ–‡ä»¶å·²ä¿å­˜:")
        print("   - phishing_urls_data.json (å®Œæ•´æ•°æ®)")
        print("   - phishing_test_urls.txt (æµ‹è¯•URLåˆ—è¡¨)")

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"   æ€»URLæ•°é‡: {data['total_count']}")
        print(f"   æ•°æ®æ¥æº: {len(data['sources'])} ä¸ª")
        print(f"   åˆ†ç±»æ•°é‡: {len(data['categories'])} ä¸ª")

        for category, urls in data['categories'].items():
            print(f"   {category}: {len(urls)} ä¸ª")

        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹URL
        print(f"\nğŸ” ç¤ºä¾‹é’“é±¼URL:")
        for i, url in enumerate(test_urls[:10]):
            print(f"   {i+1}. {url}")

    except Exception as e:
        print(f"âŒ æ”¶é›†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()